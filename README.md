# Mayanode Real-time Monitor & Arbitrage Scanner

This project provides a real-time monitoring suite for the Mayanode blockchain, including a high-performance data ingestion pipeline, a relational database for storing block data, and a Flask web application for visualizing blocks and mempool activity. The system is designed with arbitrage use-cases in mind, achieving extremely fast data retrieval and processing.

## Key Features

*   **Real-time Block Ingestion:**
    *   `src/fetch_realtime_transactions.py`: Fetches confirmed blocks from the Mayanode API (`https://mayanode.mayachain.info/mayachain/block`) and Tendermint RPC (`https://tendermint.mayachain.info`).
    *   Supports historical catch-up (with asynchronous fetching for speed), specific range/count/target fetching, and continuous polling for new blocks.
    *   Uses `tqdm` for clear progress indication during historical fetches.
*   **Robust Data Parsing & Protobuf Handling:**
    *   `src/common_utils.py`: Handles parsing of block data from various sources.
    *   Includes native Python Protobuf decoding for `cosmos.tx.v1beta1.Tx` messages (from Tendermint RPC) using `betterproto` and pre-generated stubs (`proto/generated/pb_stubs/`).
    *   Provides logic to transform decoded Tendermint transactions into a Mayanode API-like JSON structure.
    *   Detailed Protobuf handling documented in `Docs/Python_Protobuf_Decoding_Guide_Mayanode.md`.
*   **High-Performance Relational Database:**
    *   `src/database_utils.py`: Manages a SQLite database (`mayanode_blocks.db`) with a fully relational schema designed to store all discrete components of Mayanode blocks (headers, transactions, messages, events, attributes, address links).
    *   Efficient data insertion (`insert_block_and_components`).
    *   **Optimized Block Reconstruction:** The `reconstruct_block_as_mayanode_api_json` function reconstructs the full Mayanode API JSON structure from the relational database with extremely high performance (typically **5-9 milliseconds per block** on tested hardware).
    *   Utilizes SQLite's Write-Ahead Logging (WAL) mode and appropriate indexes for improved concurrency and query speed.
*   **Flask Web Application (`app.py`):**
    *   Provides a web interface (`templates/latest_blocks.html`) to monitor the Mayanode blockchain.
    *   **Latest Blocks Display:** Dynamically loads and displays the 10 newest blocks, automatically fetching newer blocks without full page reloads. Block details (header, transactions, messages, events) are collapsible for easy viewing.
    *   **Live Mempool Activity:** Displays a live feed of transactions from the mempool, refreshing periodically.
    *   **Responsive Design:** UI adapts to different screen sizes.
    *   **API Endpoints:**
        *   `/api/latest-blocks-data`: Serves the 10 latest blocks (reconstructed).
        *   `/api/blocks-since/<height>`: Serves blocks newer than the given height.
        *   `/api/mempool`: Serves current mempool transaction data.
*   **Performance for Arbitrage:** The data retrieval and block reconstruction pipeline is optimized for speed, making the system suitable for applications requiring low-latency access to blockchain data, such as arbitrage bots.

## Project Structure

(Refer to the detailed structure in the `README.md` provided in the initial custom instructions - this section will be expanded based on that and current state)

-   `app.py`: The Flask web application.
-   `src/`:
    -   `api_connections.py`: Functions for interacting with Mayanode and Tendermint RPC APIs.
    -   `common_utils.py`: Core data parsing, Protobuf decoding, and utility functions.
    -   `database_utils.py`: Database schema, connection, insertion, and querying logic.
    -   `fetch_realtime_transactions.py`: Script for fetching and storing block data.
    -   `preprocess_ai_data.py`: (Future Work) For preparing data for the AI model.
    -   `model.py`: (Future Work) Definition of the generative AI model.
    -   `train_model.py`: (Future Work) Script for training the AI model.
-   `proto/`:
    -   `src/`: Source `.proto` files (Cosmos SDK, gogoproto, googleapi, etc.).
    -   `generated/pb_stubs/`: Python stubs generated by `betterproto`.
-   `templates/`: HTML templates for the Flask application.
-   `scripts/`: Utility and testing scripts.
-   `Docs/`: Project documentation, including:
    -   `Implementation Plan.md`: Detailed project phases and tasks.
    -   `Project Requirements.md`: Functional and non-functional requirements.
    -   `Python_Protobuf_Decoding_Guide_Mayanode.md`: Guide to Protobuf setup and decoding.
-   `mayanode_blocks.db`: SQLite database file (created on first run of `fetch_realtime_transactions.py` or `app.py`).
-   `requirements.txt`: Python dependencies.
-   `.gitignore`: Specifies intentionally untracked files.
-   `README.md`: This file.

## Setup & Running

1.  **Clone the repository.**
2.  **Create and activate a Python virtual environment:**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate 
    ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Initialize the Database (Fetch some blocks):**
    Run the data fetching script. To get the latest 1000 blocks, for example:
    ```bash
    python src/fetch_realtime_transactions.py --historical-catchup 1000
    ```
    Or, to start continuous polling (will also fill gaps from the latest DB block to latest API block):
    ```bash
    python src/fetch_realtime_transactions.py
    ```
    The database file `mayanode_blocks.db` will be created in the project root.

5.  **Run the Flask Web Application:**
    ```bash
    python app.py
    ```
    The application will be available at `http://0.0.0.0:5001` (or `http://localhost:5001`).

## Future Work: Generative Block Prediction Model

The next phase of this project involves developing an AI model (likely Transformer-based) to predict sequences of blocks. This will involve:
-   Preprocessing the stored block data (`src/preprocess_ai_data.py`).
-   Defining and training a generative model (`src/model.py`, `src/train_model.py`).
-   Developing evaluation and real-time inference suites.

## Contributing

(Contributions are welcome. Please follow standard Git practices: fork, branch, commit, push, and create a Pull Request.)

## License

(Specify your license, e.g., MIT License)