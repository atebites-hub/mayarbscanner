# Project Scratchpad

## Background and Motivation
The user wants to create a scanner for arbitrage opportunities on Maya Protocol. This involves fetching real-time transaction data, identifying potential arbitrage opportunities by comparing input and output values against market prices, and then potentially simulating or executing trades.

Initial focus is on fetching and processing transaction data correctly.
**Update for Task 1.2 (Major Revision):** The requirement has changed significantly. The goal is now to fetch **all actions (transactions) from Maya Protocol's Midgard API that have occurred within the last 24 hours**. These actions will then be processed and saved.
**Further Expansion:** The project will also include:
1.  **Real-time Transaction Streaming:** Continuously fetch the latest confirmed and pending transactions from Midgard.
2.  **Pending Block Construction:** Group incoming pending transactions into a conceptual "pending block."
3.  **Flask Web Application:** Display the 24-hour historical data, the real-time confirmed transactions, and the pending block data in a web interface.
4.  **AI Data Preprocessing:** Prepare the collected data for input into machine learning models to identify arbitrage opportunities.

## Key Challenges and Analysis
- Accessing and integrating data from multiple APIs (Maya, Uniswap, CoinGecko).
- Handling potentially large volumes of historical and real-time data.
- Ensuring the data preprocessing pipeline is robust and efficient.
- Setting up a reliable mempool monitoring mechanism.
- Installing and managing dependencies like Python and PyTorch.
- **API Interaction**: Understanding the Midgard API, its endpoints, rate limits, and data structures is crucial.
- **Data Parsing**: Extracting relevant information (assets, amounts, transaction IDs, types) from complex JSON responses.
- **Arbitrage Logic**: Defining what constitutes an arbitrage opportunity and how to calculate potential profit (Î”X), considering fees and slippage. This will require price data.
- **Real-time Processing**: Efficiently handling incoming data to identify opportunities quickly.
- **Midgard API Behavior for Specific Block Data**: The primary challenge for fetching transactions from a *specific* historical block (e.g., `last_confirmed_height - 1`) is that the Midgard `/v2/actions` endpoint has proven difficult to use for this exact purpose.
    *   Using the `height` parameter with `offset` for pagination does not strictly filter by that height; it returns actions from various heights, requiring client-side filtering.
    *   Even a `limit=1` query with a `height` parameter (`/actions?height=H&limit=1`) seems unreliable for confirming that block `H` truly contains an action *from block H*. The API appears to sometimes return the closest available action from a *different, older* height if block `H` is empty or its actions are not primary in some internal Midgard index for that query. This makes it hard to reliably identify a `target_block_height` whose transactions can then be fetched.
    *   Attempting a general scan (no `height` filter, using `offset` and `limit`) for `target_block_height = last_confirmed_height - 1` often fails because the newest actions returned by `/actions?offset=0` are already from blocks *older* than `last_confirmed_height - 1`. This causes the client-side scan (which stops if `action.height < target_block_height`) to terminate on the first page, finding no actions for the target.
- **Pagination**: Ensuring all transactions for a target block are fetched, even if they span multiple API response pages. The Midgard API returns actions newest first, so to find an older target block, pagination must proceed until actions older than the target are encountered.
- **Testing Environment Consistency**: Ensuring that file edits are immediately reflected when test scripts are run.
- **Empty Data Handling**: Correctly creating output files (e.g., empty CSV with headers) when no relevant data is found for a query (e.g., no transactions in a specific block).
- **Fetching 24-Hour Data**: Efficiently retrieving all actions from the Midgard API within the last 24 hours. This will likely involve:
    *   Calculating a start timestamp (current time - 24 hours).
    *   Paginating through the `/v2/actions` endpoint (newest first).
    *   Client-side filtering of actions based on their `date` (timestamp) field to include only those within the 24-hour window.
    *   Stopping pagination once actions older than the 24-hour window are encountered.
- **Handling Large Data Volume**: The 24-hour window might contain a large number of actions, requiring efficient processing and memory management.
- **Real-time Data Streaming**: Implementing a robust and efficient mechanism to continuously fetch new data (both confirmed and pending actions) from Midgard. This might involve careful management of polling intervals or exploring if Midgard offers any push/subscription mechanisms (unlikely for REST API). This will require threading for non-blocking polling.
- **Pending Block Logic**: Defining the structure and update rules for a "pending block" composed of transactions that are not yet confirmed. This includes how to handle transactions that confirm or expire.
- **Flask Application Development**: 
    *   Designing appropriate API endpoints.
    *   Managing application state if the Flask app needs to serve live, continuously updating data.
    *   Implementing a user-friendly frontend that can display both historical and real-time data effectively.
    *   Ensuring asynchronous operations if needed to prevent blocking while serving live updates.
- **Comprehensive Testing**: Significantly refactoring `test_phase1_completion.py` to cover the 24-hour fetch, the new real-time streaming logic, pending block construction, and the Flask application's basic functionality. This will likely require advanced mocking for streaming behaviors.
- **AI Data Preprocessing**:
    *   **Feature Selection**: Identifying which data points from the fetched transactions (e.g., input/output assets, amounts, transaction types, fees, timestamps) are most relevant for predicting arbitrage. Integrating external price feeds might be necessary.
    *   **`arb_ID` Definition**: Clarifying what constitutes an "arbitrage ID" in the context of Maya Protocol transactions. This might involve identifying specific patterns (e.g., cyclic trades) or focusing on individual swap profitability.
    *   **Sequence Construction**: Defining an appropriate sequence length `M` for time-series analysis. Each sequence should represent a series of market events.
    *   **Categorical Data Handling**: Effectively encoding categorical data like asset names (e.g., `BTC.BTC`, `ETH.ETH`) using techniques like one-hot encoding or learnable embeddings.
    *   **Normalization**: Scaling numerical features to a consistent range (e.g., using `sklearn.preprocessing.MinMaxScaler` or `StandardScaler`).
    *   **Data Structure for Model Input**: Ensuring the preprocessed data is in a format suitable for PyTorch models (e.g., tensors).

## High-level Task Breakdown
(Revised significantly to reflect new requirements)

*   **Phase 1: Core Data Engine and Initial Display**
    *   1.1. Setup Python Environment & Dependencies (Pandas, Requests, Flask) - **DONE** (Flask to be added if not present)
    *   1.2. Fetch All Transactions from Blocks with Activity in the Last 24 Hours - **DONE**
        *   1.2.1. Connect to Midgard API - **DONE**
        *   1.2.2 (New Strategy): In `fetch_realtime_transactions.py`, implement logic to fetch actions page by page from the Midgard `/v2/actions` endpoint. - **DONE**
        *   1.2.3 (New Strategy): Continue fetching pages as long as the timestamp of the actions (`date` field) is within the last 24 hours. Stop when actions older than 24 hours are encountered or no more actions are returned. - **DONE**
        *   1.2.4 (New Strategy): Collect all actions that fall within the 24-hour window. - **DONE**
        *   1.2.5 (New Strategy): Parse all collected actions using `parse_action`. - **DONE**
        *   1.2.6 (New Strategy): Store all parsed transactions into a Pandas DataFrame. - **DONE**
        *   1.2.7 (New Strategy): Save the DataFrame to `data/historical_24hr_maya_transactions.csv`. Handle empty results gracefully. - **DONE**
    *   1.3. Real-time Transaction Streaming and Pending Block Construction - **DONE**
        *   1.3.1. Design Data Structures: Define Python classes or dictionaries for representing confirmed actions/blocks and "pending blocks" (collections of pending transactions with relevant metadata). - **DONE** (Initial structures in `realtime_stream_manager.py` created, `common_utils.py` for parsing also created).
        *   1.3.2. Confirmed Actions Stream: Implement a function/module (e.g., in a new `realtime_stream_manager.py` or extending `mempool_monitor.py`) to continuously poll Midgard for *newly confirmed* actions (actions with status 'success' or 'refund' that are more recent than the last seen confirmed action). - **DONE** (Implemented `poll_confirmed_actions` in `RealtimeStreamManager`).
        *   1.3.3. Pending Actions Stream: Adapt/reuse `mempool_monitor.py` logic or integrate into `realtime_stream_manager.py` to continuously poll Midgard for *pending* actions. - **DONE** (Implemented `poll_pending_actions` and `_prune_stale_pending_actions` in `RealtimeStreamManager`).
        *   1.3.4. Pending Block Management: Develop logic to:
            *   Add new pending actions to the "pending block" data structure. - **DONE** (Covered by `poll_pending_actions`)
            *   Remove actions from the "pending block" if they are later seen as confirmed (in 1.3.2 stream) or if they are assumed to have expired/failed (requires criteria, e.g., age). - **DONE** (Confirmed removal in `poll_confirmed_actions`, expiry in `_prune_stale_pending_actions`).
        *   1.3.5. Data Aggregation: Maintain in-memory collections of recent confirmed actions and the current pending block, ready for access by the Flask app. - **DONE** (`RealtimeStreamManager` stores and provides getters).
        *   1.3.6 Implement Threaded Polling: Enhance `RealtimeStreamManager` to use threads for continuous, non-blocking polling of confirmed and pending actions. - **DONE** (Implemented threaded polling in `RealtimeStreamManager`).
    *   1.4. Flask Web Application for Data Display - **DONE**
        *   1.4.1. Setup Basic Flask App: Create a new `app.py` with a basic Flask application. Install Flask dependency. - **DONE**
        *   1.4.2. Data Access Layer: Implement functions within the Flask app or a helper module to access:
            *   The 24-hour historical data (from the CSV file generated in 1.2.7). - **DONE**
            *   The live stream of confirmed actions (from 1.3.5). - **DONE**
            *   The current "pending block" data (from 1.3.5). - **DONE**
        *   1.4.3. Flask Endpoints: Create API endpoints:
            *   `/historical-24hr`: Returns the 24-hour transaction data (e.g., as JSON). - **DONE**
            *   `/live-confirmed`: Returns recent confirmed transactions. - **DONE**
            *   `/live-pending`: Returns the current pending block transactions. - **DONE**
        *   1.4.4. Frontend Design (Simple): Create basic HTML templates (`templates` folder) and static files (`static` folder). - **DONE**
            *   A page to display the 24-hour data (e.g., in a table). - **DONE** (`index.html` created)
            *   A section/page to display live confirmed transactions. - **DONE** (placeholders in `index.html`)
            *   A section/page to display pending transactions. - **DONE** (placeholders in `index.html`)
        *   1.4.5. Frontend Data Fetching: Use JavaScript (e.g., `fetch` API with `setInterval` or a more robust solution like Server-Sent Events if time permits) to poll the Flask endpoints and update the display dynamically for live data. - **DONE**
    *   1.5. Test Suite Enhancement
        *   1.5.1. Test 24-Hour Fetch: Refactor/rewrite `test_task_1_2_realtime_data` in `test_phase1_completion.py` to validate the 24-hour data fetching (Task 1.2), checking for correct timestamp filtering, data integrity, and CSV output. - **DONE**
        *   1.5.2. Test Real-time Streaming (Basic): Design and implement new tests for the core logic of fetching confirmed and pending actions (Task 1.3.2, 1.3.3). This might involve mocking `requests.get` to simulate API responses and checking if the streaming functions process them correctly. Testing continuous polling and pending block updates will be challenging and might be simplified initially.
        *   1.5.3. Test Flask App (Basic): Add new tests (possibly in `test_app.py` or `test_phase1_completion.py`) to check if Flask endpoints (Task 1.4.3) are reachable and return expected data formats (e.g., JSON responses, correct status codes).
    *   1.6. AI Data Preprocessing
        *   1.6.1. Define Feature Set for AI Model: Identify relevant features from `historical_24hr_maya_transactions.csv` (e.g., `in_asset`, `in_amount`, `out_asset`, `out_amount`, `fees`, `type`, `date`). Consider if external price data is needed and how it would be integrated at this stage.
        *   1.6.2. Implement Feature Engineering and Normalization:
            *   Handle asset names (e.g., map to unique integer IDs, then plan for embeddings later in the model).
            *   Convert timestamps to a numerical representation (e.g., seconds since epoch or time differences).
            *   Normalize numerical features (amounts, engineered time features).
            *   Create a preliminary definition of what an `arb_ID` or a sequence representing a potential arbitrage event might look like based on available transaction data.
        *   1.6.3. Develop Sequence Generation Logic: Write functions to take the processed transaction data and create sequences of length `M` (e.g., `M=10` transactions). Each item in a sequence would be a vector of features from a single transaction.
        *   1.6.4. Prepare Data for PyTorch Models: Ensure the output of sequence generation can be easily converted into PyTorch tensors (e.g., list of lists of floats, NumPy arrays).
        *   1.6.5. Initial Data Preprocessing Script: Create a new script (e.g., `src/preprocess_ai_data.py`) that loads `data/historical_24hr_maya_transactions.csv`, applies the feature engineering, normalization, and sequence generation, and can save or return the processed data.

*   **Phase 2: Arbitrage Identification** (Will use data from Phase 1)
    *   (Sub-tasks remain largely the same as previously defined, but will operate on the new data sources)
*   **Phase 3: Reporting and Simulation (Future Scope)**
    *   (Sub-tasks remain as previously defined)


## Project Status Board

**Current Overall Progress: Phase 1 - Core Data Engine and Initial Display, including AI Data Preprocessing, is COMPLETE. Ready to start Phase 2.**

*   [x] **Phase 1: Core Data Engine and Initial Display**
    *   [x] 1.1. Setup Python Environment & Dependencies (Pandas, Requests, Flask)
    *   [x] 1.2. Fetch All Transactions from Blocks with Activity in the Last 24 Hours
        *   [x] 1.2.1. Connect to Midgard API
        *   [x] 1.2.2 (New Strategy): Implement paged fetching in `fetch_realtime_transactions.py`.
        *   [x] 1.2.3 (New Strategy): Implement 24-hour timestamp filtering.
        *   [x] 1.2.4 (New Strategy): Collect actions within 24-hour window.
        *   [x] 1.2.5 (New Strategy): Parse collected actions.
        *   [x] 1.2.6 (New Strategy): Store in DataFrame.
        *   [x] 1.2.7 (New Strategy): Save to `data/historical_24hr_maya_transactions.csv`.
    *   [x] 1.3. Real-time Transaction Streaming and Pending Block Construction
        *   [x] 1.3.1. Design Data Structures for confirmed and pending blocks.
        *   [x] 1.3.2. Implement stream for newly confirmed actions.
        *   [x] 1.3.3. Implement stream for pending actions.
        *   [x] 1.3.4. Develop pending block management logic.
        *   [x] 1.3.5. Implement in-memory data aggregation for Flask.
        *   [x] 1.3.6 Implement Threaded Polling in `RealtimeStreamManager`.
    *   [x] 1.4. Flask Web Application for Data Display
        *   [x] 1.4.1. Setup Basic Flask App (`app.py`, install Flask)
        *   [x] 1.4.2. Implement Data Access Layer for Flask.
        *   [x] 1.4.3. Create Flask API Endpoints.
        *   [x] 1.4.4. Design Simple Frontend (HTML templates, static files).
        *   [x] 1.4.5. Implement Frontend Data Fetching and Dynamic Updates.
    *   [x] 1.5. Test Suite Enhancement
        *   [x] 1.5.1. Refactor/Rewrite tests for 24-Hour Fetch logic.
        *   [x] 1.5.2. Design and implement tests for Real-time Streaming logic.
        *   [x] 1.5.3. Design and implement tests for Flask App endpoints.
    *   [x] 1.6. AI Data Preprocessing
        *   [x] 1.6.1. Define Feature Set for AI Model
        *   [x] 1.6.2. Implement Feature Engineering and Normalization
        *   [x] 1.6.3. Develop Sequence Generation Logic
        *   [x] 1.6.4. Prepare Data for PyTorch Models
        *   [x] 1.6.5. Initial Data Preprocessing Script
*   [ ] **Phase 2: Arbitrage Identification** (All sub-tasks pending)
*   [ ] **Phase 3: Reporting and Simulation** (All sub-tasks pending)

## Executor's Feedback or Assistance Requests

Completed Task 1.2 (24-Hour Historical Data Fetch) and Task 1.5.1 (Update Test Suite for 24-Hour Fetch).
- `src/fetch_realtime_transactions.py` modified for 24-hour data fetching.
- `src/test_phase1_completion.py` updated to test the new 24-hour fetching logic.

Completed Task 1.3.1 (Design Data Structures for Streaming):
- Created `src/common_utils.py` with `parse_action` and `DF_COLUMNS`.
- Updated `src/fetch_realtime_transactions.py` to use `common_utils.py`.
- Created `src/realtime_stream_manager.py` with `RealtimeStreamManager` class structure and data stores.

Completed Task 1.3.2 (Confirmed Actions Stream):
- Implemented `poll_confirmed_actions` method in `RealtimeStreamManager`.

Completed Task 1.3.3 (Pending Actions Stream):
- Implemented `poll_pending_actions` method in `RealtimeStreamManager`, including logic for pruning stale actions.

Tasks 1.3.4 (Pending Block Management) and 1.3.5 (Data Aggregation) are considered complete as their core logic is integrated into the `RealtimeStreamManager` and its polling methods.

Completed Task 1.3.6 (Implement Threaded Polling):
- Implemented threaded, continuous polling in `RealtimeStreamManager` using `start_streaming` and `stop_streaming` methods.

Task 1.3 is now complete.

Verified Task 1.4.1 (Basic Flask App Setup) is complete.

Completed Task 1.4.2 (Data Access Layer):
- Integrated `RealtimeStreamManager` into `src/app.py`.
- Added `get_historical_transactions()` function to load CSV data.
- Implemented `atexit` handler for graceful shutdown of the stream manager.

Completed Task 1.4.3 (Flask Endpoints):
- Added `/api/historical-24hr`, `/api/live-confirmed`, and `/api/live-pending` endpoints to `src/app.py`.

Completed Task 1.4.4 (Frontend Design):
- Created `src/templates/index.html` with basic structure.
- Created `src/static/style.css` for styling.
- Created placeholder `src/static/script.js`.
- Updated `src/app.py` to serve `index.html`.

Completed Task 1.4.5 (Frontend Data Fetching):
- Implemented JavaScript in `src/static/script.js` to fetch and display data from API endpoints, including periodic updates for live data.

Task 1.4 is now complete.
Proceeding to Task 1.5.2: Test Real-time Streaming logic.

Completed Task 1.5.2: Test Real-time Streaming logic.
- Enhanced `test_task_1_3_realtime_stream_manager` in `src/test_phase1_completion.py` with more detailed scenarios for `RealtimeStreamManager`'s polling methods (confirmed and pending actions), including edge cases and error handling.
- Improved threading tests to verify that polling methods are actively called.

Completed Task 1.5.3: Test Flask App endpoints.
- Added `test_task_1_5_3_flask_app_endpoints` to `src/test_phase1_completion.py`.
- This test uses a Flask test client and mocking to verify the behavior of all API endpoints (`/`, `/api/historical-24hr`, `/api/live-confirmed`, `/api/live-pending`), checking status codes, content types, and data integrity.

All tasks for Phase 1 are now complete.

The Flask application is now working correctly after addressing some client-side JavaScript issues related to URL construction and DOM manipulation when handling empty API responses. The application successfully loads historical data and displays live confirmed and pending transactions.

Ready to proceed to Phase 2: Arbitrage Identification.
**Planner Update:** Phase 1 is being extended to include AI Data Preprocessing tasks.

**Executor Update (Task 1.6 - AI Data Preprocessing):**
Completed all sub-tasks for Task 1.6:
- **1.6.1 (Define Feature Set):** Identified key features from `historical_24hr_maya_transactions.csv` for the initial model: `date`, `status`, `type`, `pools`, `in_asset`, `in_amount`, `out_asset`, `out_amount`, `swap_liquidity_fee`, `swap_slip_bps`, `swap_network_fee_asset`, `swap_network_fee_amount`, `transaction_id`.
- **1.6.2 (Feature Engineering & Normalization):** Implemented in `src/preprocess_ai_data.py`.
    - Loaded data, converted amounts/dates to numeric.
    - Created mappings for categorical features (assets, types, status) to integer IDs, saved to JSON files in `data/processed_ai_data/` (e.g., `asset_to_id.json`). Handled `PAD` and `UNK` tokens.
    - Engineered time features: `timestamp_norm` (from epoch seconds) and `time_delta_norm` (difference from previous transaction).
    - Applied `MinMaxScaler` to numerical features (`_norm` suffixed columns), scaler saved to `data/processed_ai_data/scaler.pkl`.
- **1.6.3 (Sequence Generation Logic):** Implemented `generate_sequences` function in `src/preprocess_ai_data.py`.
    - Creates sequences of length `M=10`.
    - Handles padding for sequences at the beginning of the dataset.
    - Returns a 3D NumPy array and corresponding transaction IDs.
- **1.6.4 (Prepare for PyTorch):** The output NumPy array from sequence generation is directly usable for PyTorch models.
- **1.6.5 (Initial Data Preprocessing Script):** `src/preprocess_ai_data.py` is now a runnable script that performs all preprocessing steps: loads raw data, engineers features, generates sequences, and saves the processed sequences (`sequences.npy`), transaction IDs (`sequence_transaction_ids.json`), mappings, and the scaler to the `data/processed_ai_data/` directory.

Next step would be to run `src/preprocess_ai_data.py` to generate the processed data files, then move to Phase 2. The script also needs testing.
**Executor Update:** Successfully ran `src/preprocess_ai_data.py`. All processed data files (sequences, mappings, scaler) have been generated in `data/processed_ai_data/`. Phase 1 is now complete.

## Lessons

*   Midgard API `/v2/actions` endpoint is paginated with a default/max limit of 50 actions per call. Use `limit` and `offset` parameters for pagination.
*   Midgard API `/v2/health` endpoint provides `lastAggregated.height` which can be used as the latest confirmed block height.
*   Actions fetched from `/v2/actions` are generally sorted by height (descending) and then by date/time within the block.
*   When targeting transactions from a *specific older* block height using pagination (newest first API results), it's crucial to continue fetching pages as long as the actions are newer than or contemporaneous with the target block. Stop pagination once actions *older* than the target block height are encountered.
*   Verify exact string matches (including trailing spaces or additional words like "API") when writing checks in test scripts that parse stdout.
*   Pandas `df.to_csv()` on an empty DataFrame creates a file with only the header row (assuming `index=False` and `header=True`). It's good practice to explicitly define columns for the DataFrame to ensure consistent CSV headers, especially if the DataFrame might be empty.
*   **Testing Environment**: Discrepancies between local file state and the state used by automated test runners can lead to confusing test failures (e.g., debug prints not appearing, file creation behaving unexpectedly). Ensure the environment is using the latest code for tests.
*   When installing Python on macOS, ensure Homebrew is installed first (`/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"`), then install Python (`brew install python`).
*   Install Python packages using `python3 -m pip install <package_name>`. Be mindful of which `python3` is being used if multiple versions are present. The desired Python (e.g., from Homebrew) should be prioritized in the system PATH or called explicitly (e.g. `/opt/homebrew/bin/python3 -m pip install ...`).
*   PyTorch installation via pip: `pip install torch torchvision torchaudio`.
*   The correct Maya Protocol API endpoint for pools is `https://mayanode.mayachain.info/mayachain/pools`. The initially provided `api.mayaprotocol.com` was incorrect or not resolvable.
*   When generating mock data with pandas, ensure `os.makedirs(exist_ok=True)` or an explicit check `if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)` is used to create output directories to prevent errors if the script is run multiple times or the directory doesn't exist.
*   For long-running simulation scripts (like a mempool monitor), include a `try...except KeyboardInterrupt` block to handle graceful shutdown when the user presses Ctrl+C.
*   When preprocessing data for sequences: clearly define features to include. Convert timestamps to numerical representations (e.g., Unix epoch seconds) before scaling. Map categorical string features to numerical indices. For unique IDs like `arb_ID`, they are generally not embedded directly for generalization; instead, use them as identifiers or extract structured features if available. Use `torch.tensor()` to convert NumPy arrays to PyTorch tensors.
*   **Maya Protocol Midgard API:** The correct public Midgard API endpoint for Maya Protocol is `https://midgard.mayachain.info/v2`. Using a generic or THORChain-specific Midgard endpoint (like one from `ninerealms.com`) will not work for Maya Protocol specific data.
*   For managing Python dependencies and avoiding conflicts with system Python (especially on macOS with Homebrew), use virtual environments:
  - Create: `python3 -m venv .venv`
  - Install packages: `.venv/bin/python -m pip install <package_name>`
  - Run scripts: `.venv/bin/python your_script.py`
*   Ensure scripts are run with `python3` (or the specific venv python like `.venv/bin/python`) if `python` is not aliased or available. If `ModuleNotFound` errors occur, ensure all dependencies (e.g., `pandas`, `requests`, `scikit-learn`, `torch`) are installed into the active virtual environment.
*   **Output Buffering with subprocess:** When a Python script run via `subprocess.run` appears to produce no `stdout` or `stderr` despite working correctly when run directly, it might be due to output buffering. Add `flush=True` to `print()` statements in the child script to ensure output is written immediately and captured by the parent process, especially if the child script is terminated by a timeout.
*   **Midgard API `/v2/actions` Filtering:** When using the Maya Midgard API (`https://midgard.mayachain.info/v2`), attempting to filter `/v2/actions` with both `status=pending` and `type=swap` server-side *may* lead to issues (like the 400 error seen with a different endpoint, or computationally expensive results). It's safer and verified to fetch by `type=swap` and then filter for `action.get('status') == 'pending'` on the client side.

## User Specified Information

<user_provided_data>
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- If there are vulnerabilities that appear in the terminal, run npm audit before proceeding
- Always ask before using the -force git command
</user_provided_data> 